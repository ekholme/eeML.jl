var documenterSearchIndex = {"docs":
[{"location":"#eeML","page":"Home","title":"eeML","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for eeML.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#eeML.KnnClassification","page":"Home","title":"eeML.KnnClassification","text":"KnnClassification\n\nA k-Nearest Neighbors classifier model. The struct is mutable and stores the training data.\n\nFields\n\nX::AbstractMatrix{<:Real}: The matrix of training features.\ny::AbstractVector{<:Any}: The vector of training labels.\n\n\n\n\n\n","category":"type"},{"location":"#eeML.KnnClassification-Tuple{}","page":"Home","title":"eeML.KnnClassification","text":"KnnClassification()\n\nConstructs an untrained KnnClassification model. The training data fields X and y are initialized as empty and will be populated by the fit! function.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.KnnRegression","page":"Home","title":"eeML.KnnRegression","text":"KnnRegression\n\nA k-Nearest Neighbors regressor model. The struct is mutable and stores the training data.\n\nFields\n\nX::AbstractMatrix{<:Real}: The matrix of training features.\ny::AbstractVector{<:Real}: The vector of training target values.\n\n\n\n\n\n","category":"type"},{"location":"#eeML.KnnRegression-Tuple{}","page":"Home","title":"eeML.KnnRegression","text":"KnnRegression()\n\nConstructs an untrained KnnRegression model. The training data fields X and y are initialized as empty and will be populated by the fit! function.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.LinearRegression","page":"Home","title":"eeML.LinearRegression","text":"LinearRegression\n\nA linear regression model object. The struct is mutable so that the fit! function can update the coefficients β.\n\nFields\n\nβ::Vector{Float64}: The coefficients of the linear model. It is empty until fit! is called.\n\n\n\n\n\n","category":"type"},{"location":"#eeML.LinearRegression-Tuple{}","page":"Home","title":"eeML.LinearRegression","text":"LinearRegression()\n\nConstructs an untrained LinearRegression model. The coefficients β are initialized as an empty vector and will be populated by the fit! function.\n\nExamples\n\njulia> model = LinearRegression()\nLinearRegression(Float64[])\n\n\n\n\n\n","category":"method"},{"location":"#eeML._check_knn_inputs-Tuple{AbstractMatrix{<:Real}, AbstractMatrix{<:Real}, Int64}","page":"Home","title":"eeML._check_knn_inputs","text":"_check_knn_inputs(train_x, test_x, k)\n\nInternal helper to perform input validation for k-NN prediction.\n\nChecks that the number of features in the test set matches the training set, and that k is a valid number (i.e., between 1 and the number of training samples).\n\n\n\n\n\n","category":"method"},{"location":"#eeML.accuracy-Tuple{AbstractVector, AbstractVector}","page":"Home","title":"eeML.accuracy","text":"accuracy(y::AbstractVector, ŷ::AbstractVector)\n\nCalculate the classification accuracy.\n\nAccuracy is the proportion of correct predictions, calculated as (number of correct predictions) / (total number of predictions).\n\nArguments\n\ny::AbstractVector: The vector of true labels.\nŷ::AbstractVector: The vector of predicted labels.\n\nReturns\n\nFloat64: The accuracy score, a value between 0.0 and 1.0.\n\nExamples\n\njulia> accuracy([1, 2, 3, 4], [1, 2, 4, 4])\n0.75\n\n\n\n\n\n","category":"method"},{"location":"#eeML.binary_crossentropy-Tuple{AbstractVector{<:Integer}, AbstractVector{<:Real}}","page":"Home","title":"eeML.binary_crossentropy","text":"binary_crossentropy(y::AbstractVector{<:Integer}, ŷ::AbstractVector{<:Real})\n\nCalculate the binary cross-entropy loss, commonly used for binary classification.\n\nLoss is calculated as -mean(y .* log.(ŷ) .+ (1 .- y) .* log.(1 .- ŷ)). A small epsilon eps is used to avoid log(0).\n\nArguments\n\ny::AbstractVector{<:Integer}: The vector of true binary labels (0 or 1).\nŷ::AbstractVector{<:Real}: The vector of predicted probabilities (between 0 and 1).\n\nReturns\n\nFloat64: The binary cross-entropy loss.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.fit!-Tuple{KnnClassification, AbstractMatrix{<:Real}, AbstractVector}","page":"Home","title":"eeML.fit!","text":"fit!(model::KnnClassification, X::AbstractMatrix{<:Real}, y::AbstractVector{<:Any})\n\n\"Trains\" the KnnClassification model by storing the feature matrix X and label vector y.\n\nIn k-NN, fitting is a lazy process that simply consists of memorizing the training data.\n\nArguments\n\nmodel::KnnClassification: The model to be trained.\nX::AbstractMatrix{<:Real}: The matrix of training features.\ny::AbstractVector{<:Any}: The vector of training labels.\n\nReturns\n\nKnnClassification: The trained model containing the training data.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.fit!-Tuple{KnnRegression, AbstractMatrix{<:Real}, AbstractVector{<:Real}}","page":"Home","title":"eeML.fit!","text":"fit!(model::KnnRegression, X::AbstractMatrix{<:Real}, y::AbstractVector{<:Real})\n\n\"Trains\" the KnnRegression model by storing the feature matrix X and target vector y.\n\nIn k-NN, fitting is a lazy process that simply consists of memorizing the training data.\n\nArguments\n\nmodel::KnnRegression: The model to be trained.\nX::AbstractMatrix{<:Real}: The matrix of training features.\ny::AbstractVector{<:Real}: The vector of training target values.\n\nReturns\n\nKnnRegression: The trained model containing the training data.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.fit!-Tuple{LinearRegression, AbstractMatrix{<:Real}, AbstractVector{<:Real}}","page":"Home","title":"eeML.fit!","text":"fit!(model::LinearRegression, X::AbstractMatrix{<:Real}, y::AbstractVector{<:Real})\n\nTrain the LinearRegression model using the feature matrix X and target vector y.\n\nThis function mutates the model object by updating its β coefficients. Two fitting methods are available:\n\n:closed_form: (Default) Solves the least-squares problem Xβ = y using Julia's highly efficient and numerically stable \\ operator.\n:grad_descent: Uses gradient descent to minimize the mean squared error. This can be useful for very large datasets or for educational purposes.\n\nArguments\n\nmodel::LinearRegression: The model to be trained.\nX::AbstractMatrix{<:Real}: The matrix of features. It's common to include a column of ones for an intercept term.\ny::AbstractVector{<:Real}: The vector of target values.\n\nKeywords\n\nfit_method::Symbol=:closed_form: The method to use for fitting. Can be :closed_form or :grad_descent.\nkwargs...: Additional keyword arguments passed to the gradient_descent optimizer when fit_method is :grad_descent.\n\nReturns\n\nLinearRegression: The trained model.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.fit!-Tuple{LogisticRegression, AbstractMatrix{<:Real}, AbstractVector{<:Integer}}","page":"Home","title":"eeML.fit!","text":"fit!(model::LogisticRegression, X::AbstractMatrix{<:Real}, y::AbstractVector{<:Integer}; kwargs...)\n\nTrain the LogisticRegression model using the feature matrix X and target vector y.\n\nThis function uses gradient_descent to find the optimal coefficients β that minimize the binary cross-entropy loss.\n\nArguments\n\nmodel::LogisticRegression: The model to be trained.\nX::AbstractMatrix{<:Real}: The matrix of features. It's common to include a column of ones for an intercept term.\ny::AbstractVector{<:Integer}: The vector of target binary labels (0 or 1).\n\nKeywords\n\nkwargs...: Keyword arguments passed directly to the gradient_descent optimizer (e.g., learning_rate, max_iter).\n\nReturns\n\nLogisticRegression: The trained model.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.gradient_descent-Tuple{Any, AbstractVector{<:Real}}","page":"Home","title":"eeML.gradient_descent","text":"gradient_descent(loss_func, initial_β; learning_rate=0.01, tol=1e-6, max_iter=1_000, verbose=false)\n\nPerform gradient descent to find the parameters β that minimize the loss_func.\n\nThis is a general-purpose optimizer. The loss_func must be a function that takes a single vector argument (the parameters β) and returns a scalar loss. For supervised learning problems, you should create a closure that captures your data (X, y).\n\nArguments\n\nloss_func: A callable function that accepts a vector β and returns a scalar loss.\ninitial_β::AbstractVector{<:Real}: The starting values for the parameters β.\n\nKeywords\n\nlearning_rate::Float64=0.01: The step size for each iteration.\ntol::Float64=1e-6: The tolerance for convergence. The algorithm stops when the L2 norm of the change in β is less than this value.\nmax_iter::Int=1_000: The maximum number of iterations.\nverbose::Bool=false: If true, prints the iteration number and loss at each step.\n\nReturns\n\nVector{Float64}: The optimized parameters β.\n\nExample\n\n1. Linear Regression\n\n# Define a mean squared error loss function for a linear model\nX = hcat(ones(100), rand(100, 1)) # Add intercept\ntrue_β = [1.5, -3.0]\ny = X * true_β + 0.2 * randn(100)\nloss(β) = sum((y - X * β).^2) / length(y)\n\n# Initial guess for parameters\ninitial_β = rand(2)\n\n# Run optimizer\nβ_optimized = gradient_descent(loss, initial_β, learning_rate=0.1);\nisapprox(β_optimized, true_β, atol=0.1)\n\n# output\n\ntrue\n\n2. Logistic Regression\n\nFor logistic regression, we create a closure around the binary cross-entropy loss. This is exactly what the fit! method for LogisticRegression does internally.\n\n# The loss function captures X and y from its environment\nloss_logistic(β) = binary_crossentropy(y_binary, sigmoid(X * β))\n\n# Then you would call the optimizer:\n# β_logistic = gradient_descent(loss_logistic, initial_β_logistic)\n\n\n\n\n\n","category":"method"},{"location":"#eeML.mode-Tuple{AbstractVector}","page":"Home","title":"eeML.mode","text":"mode(x::AbstractVector{<:Any})\n\nRetrieve the mode of the input vector. This function will only return the first mode if there are multiple modes.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.mse-Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}}","page":"Home","title":"eeML.mse","text":"mse(y::AbstractVector{<:Real}, ŷ::AbstractVector{<:Real})\n\nCalculate the Mean Squared Error (MSE) between the true values y and the predicted values ŷ.\n\nMSE is calculated as mean((y - ŷ).^2). This is a common loss function for regression problems.\n\nArguments\n\ny::AbstractVector{<:Real}: The vector of true values.\nŷ::AbstractVector{<:Real}: The vector of predicted values.\n\nReturns\n\nFloat64: The Mean Squared Error.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.predict","page":"Home","title":"eeML.predict","text":"predict(model::KnnRegression, X::AbstractMatrix{<:Real}, k::Int=5)\n\nMake predictions using a trained KnnRegression model.\n\nFor each sample in X, the prediction is the mean of the target values of its k nearest neighbors in the training data.\n\nArguments\n\nmodel::KnnRegression: The trained model containing the training data.\nX::AbstractMatrix{<:Real}: The matrix of features for which to make predictions.\nk::Int=5: The number of nearest neighbors to consider.\n\nReturns\n\nVector{Float64}: The vector of predicted values.\n\n\n\n\n\n","category":"function"},{"location":"#eeML.predict-2","page":"Home","title":"eeML.predict","text":"predict(model::KnnClassification, X::AbstractMatrix{<:Real}, k::Int=5)\n\nMake predictions using a trained KnnClassification model.\n\nFor each sample in X, the prediction is the most frequent class (mode) among its k nearest neighbors in the training data.\n\nArguments\n\nmodel::KnnClassification: The trained model containing the training data.\nX::AbstractMatrix{<:Real}: The matrix of features for which to make predictions.\nk::Int=5: The number of nearest neighbors to consider.\n\nReturns\n\nAbstractVector: The vector of predicted labels, with the same element type as model.y.\n\n\n\n\n\n","category":"function"},{"location":"#eeML.predict-Tuple{LinearRegression, AbstractMatrix{<:Real}}","page":"Home","title":"eeML.predict","text":"predict(model::LinearRegression, X::AbstractMatrix{<:Real})\n\nMake predictions using a trained LinearRegression model.\n\nArguments\n\nmodel::LinearRegression: The trained model containing coefficients β.\nX::AbstractMatrix{<:Real}: The matrix of features for which to make predictions.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.predict-Tuple{LogisticRegression, AbstractMatrix{<:Real}}","page":"Home","title":"eeML.predict","text":"predict(model::LogisticRegression, X::AbstractMatrix{<:Real}; threshold::Union{Float64, Nothing}=0.5)\n\nMake predictions using a trained LogisticRegression model. Returns class labels by default.\n\nArguments\n\nmodel::LogisticRegression: The trained model.\nX::AbstractMatrix{<:Real}: The matrix of features for which to make predictions.\nthreshold::Union{Float64, Nothing}=0.5: The probability threshold for classifying as 1. If nothing, raw probabilities are returned.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.r_squared-Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}}","page":"Home","title":"eeML.r_squared","text":"r_squared(y::Vector{<:Real}, ŷ::Vector{<:Real})\n\nCalculate the R-squared (coefficient of determination) value.\n\nR-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It is calculated as 1 - (SS_res / SS_tot), where SS_res is the sum of squared residuals and SS_tot is the total sum of squares.\n\nArguments\n\ny::AbstractVector{<:Real}: The vector of true values.\nŷ::AbstractVector{<:Real}: The vector of predicted values.\n\nReturns\n\nFloat64: The R-squared value.\n\nExamples\n\njulia> r_squared([1, 2, 3, 4], [1.1, 1.9, 3.2, 3.8])\n0.98\n\n\n\n\n\n","category":"method"},{"location":"#eeML.rmse-Tuple{Vector{<:Real}, Vector{<:Real}}","page":"Home","title":"eeML.rmse","text":"rmse(y::AbstractVector{<:Real}, ŷ::AbstractVector{<:Real})\n\nCalculate the Root Mean Squared Error (RMSE) between the true values y and the predicted values ŷ.\n\nRMSE is calculated as sqrt(mean((y - ŷ).^2)).\n\nArguments\n\ny::AbstractVector{<:Real}: The vector of true values.\nŷ::AbstractVector{<:Real}: The vector of predicted values.\n\nReturns\n\nFloat64: The Root Mean Squared Error.\n\nExamples\n\njulia> rmse([1, 2, 3], [1.1, 2.2, 2.9])\n0.14142135623730964\n\n\n\n\n\n","category":"method"},{"location":"#eeML.sigmoid-Tuple{Real}","page":"Home","title":"eeML.sigmoid","text":"sigmoid(x)\n\nCalculate the sigmoid function 1 / (1 + exp(-x)).\n\nThis function is broadcasted element-wise for array inputs.\n\n\n\n\n\n","category":"method"}]
}
