var documenterSearchIndex = {"docs":
[{"location":"#eeML","page":"Home","title":"eeML","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for eeML.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#eeML.LinearRegression","page":"Home","title":"eeML.LinearRegression","text":"LinearRegression\n\nA linear regression model object. The struct is mutable so that the fit! function can update the coefficients β.\n\nFields\n\nβ::Vector{Float64}: The coefficients of the linear model. It is empty until fit! is called.\n\n\n\n\n\n","category":"type"},{"location":"#eeML.LinearRegression-Tuple{}","page":"Home","title":"eeML.LinearRegression","text":"LinearRegression()\n\nConstructs an untrained LinearRegression model. The coefficients β are initialized as an empty vector and will be populated by the fit! function.\n\nExamples\n\njulia> model = LinearRegression()\nLinearRegression(Float64[])\n\n\n\n\n\n","category":"method"},{"location":"#eeML.binary_crossentropy-Tuple{AbstractVector{<:Integer}, AbstractVector{<:Real}}","page":"Home","title":"eeML.binary_crossentropy","text":"binary_crossentropy(y::AbstractVector{<:Integer}, ŷ::AbstractVector{<:Real})\n\nCalculate the binary cross-entropy loss, commonly used for binary classification.\n\nLoss is calculated as -mean(y .* log.(ŷ) .+ (1 .- y) .* log.(1 .- ŷ)). A small epsilon eps is used to avoid log(0).\n\nArguments\n\ny::AbstractVector{<:Integer}: The vector of true binary labels (0 or 1).\nŷ::AbstractVector{<:Real}: The vector of predicted probabilities (between 0 and 1).\n\nReturns\n\nFloat64: The binary cross-entropy loss.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.fit!-Tuple{LinearRegression, AbstractMatrix{<:Real}, AbstractVector{<:Real}}","page":"Home","title":"eeML.fit!","text":"fit!(model::LinearRegression, X::AbstractMatrix{<:Real}, y::AbstractVector{<:Real})\n\nTrain the LinearRegression model using the feature matrix X and target vector y.\n\nThis function mutates the model object by updating its β coefficients. Two fitting methods are available:\n\n:closed_form: (Default) Solves the least-squares problem Xβ = y using Julia's highly efficient and numerically stable \\ operator.\n:grad_descent: Uses gradient descent to minimize the mean squared error. This can be useful for very large datasets or for educational purposes.\n\nArguments\n\nmodel::LinearRegression: The model to be trained.\nX::AbstractMatrix{<:Real}: The matrix of features. It's common to include a column of ones for an intercept term.\ny::AbstractVector{<:Real}: The vector of target values.\n\nKeywords\n\nfit_method::Symbol=:closed_form: The method to use for fitting. Can be :closed_form or :grad_descent.\nkwargs...: Additional keyword arguments passed to the gradient_descent optimizer when fit_method is :grad_descent.\n\nReturns\n\nLinearRegression: The trained model.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.fit!-Tuple{LogisticRegression, AbstractMatrix{<:Real}, AbstractVector{<:Integer}}","page":"Home","title":"eeML.fit!","text":"fit!(model::LogisticRegression, X::AbstractMatrix{<:Real}, y::AbstractVector{<:Integer}; kwargs...)\n\nTrain the LogisticRegression model using the feature matrix X and target vector y.\n\nThis function uses gradient_descent to find the optimal coefficients β that minimize the binary cross-entropy loss.\n\nArguments\n\nmodel::LogisticRegression: The model to be trained.\nX::AbstractMatrix{<:Real}: The matrix of features. It's common to include a column of ones for an intercept term.\ny::AbstractVector{<:Integer}: The vector of target binary labels (0 or 1).\n\nKeywords\n\nkwargs...: Keyword arguments passed directly to the gradient_descent optimizer (e.g., learning_rate, max_iter).\n\nReturns\n\nLogisticRegression: The trained model.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.gradient_descent-Tuple{Any, AbstractVector{<:Real}}","page":"Home","title":"eeML.gradient_descent","text":"gradient_descent(loss_func, initial_β; learning_rate=0.01, tol=1e-6, max_iter=1_000, verbose=false)\n\nPerform gradient descent to find the parameters β that minimize the loss_func.\n\nThis is a general-purpose optimizer. The loss_func must be a function that takes a single vector argument (the parameters β) and returns a scalar loss. For supervised learning problems, you should create a closure that captures your data (X, y).\n\nArguments\n\nloss_func: A callable function that accepts a vector β and returns a scalar loss.\ninitial_β::AbstractVector{<:Real}: The starting values for the parameters β.\n\nKeywords\n\nlearning_rate::Float64=0.01: The step size for each iteration.\ntol::Float64=1e-6: The tolerance for convergence. The algorithm stops when the L2 norm of the change in β is less than this value.\nmax_iter::Int=1_000: The maximum number of iterations.\nverbose::Bool=false: If true, prints the iteration number and loss at each step.\n\nReturns\n\nVector{Float64}: The optimized parameters β.\n\nExample\n\n1. Linear Regression\n\n# Define a mean squared error loss function for a linear model\nX = hcat(ones(100), rand(100, 1)) # Add intercept\ntrue_β = [1.5, -3.0]\ny = X * true_β + 0.2 * randn(100)\nloss(β) = sum((y - X * β).^2) / length(y)\n\n# Initial guess for parameters\ninitial_β = rand(2)\n\n# Run optimizer\nβ_optimized = gradient_descent(loss, initial_β, learning_rate=0.1);\nisapprox(β_optimized, true_β, atol=0.1)\n\n# output\n\ntrue\n\n2. Logistic Regression\n\nFor logistic regression, we create a closure around the binary cross-entropy loss. This is exactly what the fit! method for LogisticRegression does internally.\n\n# The loss function captures X and y from its environment\nloss_logistic(β) = binary_crossentropy(y_binary, sigmoid(X * β))\n\n# Then you would call the optimizer:\n# β_logistic = gradient_descent(loss_logistic, initial_β_logistic)\n\n\n\n\n\n","category":"method"},{"location":"#eeML.mse-Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}}","page":"Home","title":"eeML.mse","text":"mse(y::AbstractVector{<:Real}, ŷ::AbstractVector{<:Real})\n\nCalculate the Mean Squared Error (MSE) between the true values y and the predicted values ŷ.\n\nMSE is calculated as mean((y - ŷ).^2). This is a common loss function for regression problems.\n\nArguments\n\ny::AbstractVector{<:Real}: The vector of true values.\nŷ::AbstractVector{<:Real}: The vector of predicted values.\n\nReturns\n\nFloat64: The Mean Squared Error.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.predict-Tuple{LinearRegression, AbstractMatrix{<:Real}}","page":"Home","title":"eeML.predict","text":"predict(model::LinearRegression, X::AbstractMatrix{<:Real})\n\nMake predictions using a trained LinearRegression model.\n\nArguments\n\nmodel::LinearRegression: The trained model containing coefficients β.\nX::AbstractMatrix{<:Real}: The matrix of features for which to make predictions.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.predict-Tuple{LogisticRegression, AbstractMatrix{<:Real}}","page":"Home","title":"eeML.predict","text":"predict(model::LogisticRegression, X::AbstractMatrix{<:Real}; threshold::Union{Float64, Nothing}=0.5)\n\nMake predictions using a trained LogisticRegression model. Returns class labels by default.\n\nArguments\n\nmodel::LogisticRegression: The trained model.\nX::AbstractMatrix{<:Real}: The matrix of features for which to make predictions.\nthreshold::Union{Float64, Nothing}=0.5: The probability threshold for classifying as 1. If nothing, raw probabilities are returned.\n\n\n\n\n\n","category":"method"},{"location":"#eeML.r_squared-Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}}","page":"Home","title":"eeML.r_squared","text":"r_squared(y::Vector{<:Real}, ŷ::Vector{<:Real})\n\nCalculate the R-squared (coefficient of determination) value.\n\nR-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It is calculated as 1 - (SS_res / SS_tot), where SS_res is the sum of squared residuals and SS_tot is the total sum of squares.\n\nArguments\n\ny::AbstractVector{<:Real}: The vector of true values.\nŷ::AbstractVector{<:Real}: The vector of predicted values.\n\nReturns\n\nFloat64: The R-squared value.\n\nExamples\n\njulia> r_squared([1, 2, 3, 4], [1.1, 1.9, 3.2, 3.8])\n0.98\n\n\n\n\n\n","category":"method"},{"location":"#eeML.rmse-Tuple{Vector{<:Real}, Vector{<:Real}}","page":"Home","title":"eeML.rmse","text":"rmse(y::AbstractVector{<:Real}, ŷ::AbstractVector{<:Real})\n\nCalculate the Root Mean Squared Error (RMSE) between the true values y and the predicted values ŷ.\n\nRMSE is calculated as sqrt(mean((y - ŷ).^2)).\n\nArguments\n\ny::AbstractVector{<:Real}: The vector of true values.\nŷ::AbstractVector{<:Real}: The vector of predicted values.\n\nReturns\n\nFloat64: The Root Mean Squared Error.\n\nExamples\n\njulia> rmse([1, 2, 3], [1.1, 2.2, 2.9])\n0.14142135623730964\n\n\n\n\n\n","category":"method"},{"location":"#eeML.sigmoid-Tuple{Real}","page":"Home","title":"eeML.sigmoid","text":"sigmoid(x)\n\nCalculate the sigmoid function 1 / (1 + exp(-x)).\n\nThis function is broadcasted element-wise for array inputs.\n\n\n\n\n\n","category":"method"}]
}
